from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.executors.pool import ThreadPoolExecutor
from apscheduler.events import EVENT_JOB_ERROR, EVENT_JOB_EXECUTED
import logging
from datetime import datetime

from fans_feedback import crawl_incremental_once  # ğŸ‘ˆ ä½ çš„çˆ¬è™«å‡½æ•°

# ======================
# æ—¥å¿—
# ======================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s"
)

# ======================
# Scheduler åˆå§‹åŒ–
# ======================
executors = {
    "default": ThreadPoolExecutor(max_workers=5)
}

scheduler = BlockingScheduler(
    executors=executors,
    timezone="Asia/Shanghai"
)

# ======================
# äº‹ä»¶ç›‘å¬ï¼ˆå¯é€‰ä½†å¼ºçƒˆæ¨èï¼‰
# ======================
def job_listener(event):
    if event.exception:
        logging.error(f"ä»»åŠ¡å¼‚å¸¸: {event.job_id}")
    else:
        logging.info(f"ä»»åŠ¡å®Œæˆ: {event.job_id}")

scheduler.add_listener(
    job_listener,
    EVENT_JOB_EXECUTED | EVENT_JOB_ERROR
)

# ======================
# Job å®šä¹‰
# ======================
def crawl_job():
    logging.info("å¼€å§‹æ‰§è¡Œå¢é‡çˆ¬è™«")
    crawl_incremental_once()
    logging.info("å¢é‡çˆ¬è™«æ‰§è¡Œå®Œæˆ")

# æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
scheduler.add_job(
    crawl_job,
    trigger="interval",
    minutes=1,
    id="incremental_crawler",
    replace_existing=True,
    max_instances=1,     # ğŸš¨ é˜²æ­¢é‡å…¥
    coalesce=True        # ğŸš¨ å †ç§¯æ—¶åˆå¹¶
)

# ======================
# å¯åŠ¨
# ======================
if __name__ == "__main__":
    logging.info("SentinelEye è°ƒåº¦å™¨å¯åŠ¨")
    crawl_job()  # ğŸš€ å¯åŠ¨ç«‹åˆ»è·‘ä¸€æ¬¡
    scheduler.start()
